# version: "3.9"

# services:
#   backend:
#     build:
#       context: ./backend
#       dockerfile: Dockerfile
#     container_name: td-backend
#     ports:
#       - "8000:8000"
#     env_file: .env
#     volumes:
#       - ./backend:/app
#     command: uvicorn main:app --reload --host 0.0.0.0 --port 8000

#   frontend:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     container_name: td-frontend
#     depends_on:
#       - backend
#     ports:
#       - "8080:8080"
#     environment:
#       VITE_IN_DOCKER: "1"
#       CHOKIDAR_USEPOLLING: "true"
#     volumes:
#       - .:/usr/src/app
#       - /usr/src/app/node_modules
#     working_dir: /usr/src/app
#     command: npm run dev -- --host 0.0.0.0 --port 8080

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: td-backend
    ports:
      - "8000:8000"
    env_file: .env
    environment:
      # App DB (your Pi)
      DATABASE_URL: postgresql://dappa:bunde123123@192.168.178.157:5432/dappa

      # Vanna auto-connect to your Pi DB (used by router)
      VANNA_PG_HOST: 192.168.178.157
      VANNA_PG_DB: dappa
      VANNA_PG_USER: dappa
      VANNA_PG_PASSWORD: bunde123123
      VANNA_PG_PORT: "5432"

      # Talk to the Ollama *service* from inside Docker
      OLLAMA_HOST: http://ollama:11434

      # Persist training / uploads
      LOCAL_STORAGE: /data
      VANNA_CHROMA_DIR: /data/chroma

      # CORS for your UI
      ALLOWED_ORIGINS: http://localhost:8080,http://192.168.178.112:8080
    volumes:
      - ./backend:/app
      - api_chroma:/app/.chroma
      - data_share:/data
    depends_on:
      ollama:
        condition: service_healthy
    command: uvicorn main:app --reload --host 0.0.0.0 --port 8000
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: td-frontend
    depends_on:
      - backend
    ports:
      - "8080:8080"
    environment:
      VITE_IN_DOCKER: "1"
      CHOKIDAR_USEPOLLING: "true"
    volumes:
      - .:/usr/src/app
      - /usr/src/app/node_modules
    working_dir: /usr/src/app
    command: npm run dev -- --host 0.0.0.0 --port 8080
    restart: unless-stopped

  ollama:
    image: ollama/ollama:0.11.6
    container_name: ollama-1
    ports:
      - "11434:11434"
    environment:
      # Listen on all interfaces inside the container
      OLLAMA_HOST: "0.0.0.0:11434"
      VANNA_ALLOW_LLM_TO_SEE_DATA: "true" 
    volumes:
      - ollama_models:/root/.ollama
    # Healthcheck so backend waits until the API is up
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 5s
    restart: unless-stopped

volumes:
  api_chroma:
  data_share:
  ollama_models:
